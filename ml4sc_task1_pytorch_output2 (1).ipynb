{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data processing\n",
        "Get the electron and photon dataset into colab"
      ],
      "metadata": {
        "id": "DFcnvGiNVKOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import io\n",
        "url='https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "with requests.Session() as session:\n",
        "    r = session.get(url, stream=True)\n",
        "    r.raise_for_status()\n",
        "    with open('SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5', 'wb') as hd5:\n",
        "        for chunk in r.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
        "            hd5.write(chunk)"
      ],
      "metadata": {
        "id": "LSOXspBnZiKP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "with requests.Session() as session:\n",
        "    r = session.get(url, stream=True)\n",
        "    r.raise_for_status()\n",
        "    with open('SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', 'wb') as hd5:\n",
        "        for chunk in r.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
        "            hd5.write(chunk)"
      ],
      "metadata": {
        "id": "ak_sifzAZkIB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the necessary libraries to process and plot the dataset. As well as necessary pytorch libraries to train the model. "
      ],
      "metadata": {
        "id": "I8ODTjIFVRIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fOWF1dYvZcIi"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_electron = h5py.File('/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
        "data_photon = h5py.File('/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')"
      ],
      "metadata": {
        "id": "UoCBU5_VaBSE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [data_electron, data_photon]\n",
        "X = np.concatenate([data['/X'][:75000] for data in datasets])\n",
        "y = np.concatenate([data['/y'][:75000] for data in datasets])"
      ],
      "metadata": {
        "id": "bR8LVPStaIdr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "N3ChrPF2aLij"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class particleDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, index):\n",
        "    # Select sample\n",
        "    image = self.X[index]\n",
        "\n",
        "    X = self.transform(image)\n",
        "    return X, self.y[index]\n",
        "\n",
        "  transform = transforms.Compose([\n",
        "        transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "FrF4umROkvWc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "transformed_dataset = particleDataset(X_train, y_train)\n",
        "trainloader = DataLoader(transformed_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "transformed_test_dataset = particleDataset(X_test, y_test)\n",
        "testloader = DataLoader(transformed_test_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tyHLdNYlZiy",
        "outputId": "25af78d0-4d27-43f1-8077-6b33ba2ac3cb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EUXlHJW6LwJN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=128 * 8 * 8, out_features=256)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        \n",
        "        x = x.view(-1, 128 * 8 * 8)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xQ8LQRJxeXCS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(F.softmax(outputs, dim=1), 1)   # get predicted class\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print('[Epoch %d, Batch %5d] loss: %.3f, accuracy: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100, correct / total))\n",
        "            running_loss = 0.0\n",
        "            total = 0\n",
        "            correct = 0\n",
        "    accuracy = correct / total\n",
        "    print('Epoch %d accuracy: %.3f' % (epoch + 1, accuracy))\n",
        "    # evaluate on test set\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "            test_predictions.extend(predicted.tolist())\n",
        "            test_targets.extend(labels.tolist())\n",
        "    test_accuracy = accuracy_score(test_targets, test_predictions)\n",
        "    test_roc_auc = roc_auc_score(test_targets, test_predictions)\n",
        "    print('Epoch %d test accuracy: %.3f, test ROC AUC score: %.3f' % (epoch + 1, test_accuracy, test_roc_auc))\n",
        "filepath = \"/content/modelModule.pt\"\n",
        "torch.save(model.state_dict(), filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR8qSxgJnh3l",
        "outputId": "c93176f6-e074-4a8d-b3a2-5ca83e70eb09"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch   100] loss: 0.691, accuracy: 0.526\n",
            "[Epoch 1, Batch   200] loss: 0.677, accuracy: 0.580\n",
            "[Epoch 1, Batch   300] loss: 0.667, accuracy: 0.593\n",
            "[Epoch 1, Batch   400] loss: 0.663, accuracy: 0.604\n",
            "[Epoch 1, Batch   500] loss: 0.660, accuracy: 0.609\n",
            "[Epoch 1, Batch   600] loss: 0.661, accuracy: 0.604\n",
            "[Epoch 1, Batch   700] loss: 0.652, accuracy: 0.618\n",
            "[Epoch 1, Batch   800] loss: 0.660, accuracy: 0.605\n",
            "[Epoch 1, Batch   900] loss: 0.655, accuracy: 0.615\n",
            "[Epoch 1, Batch  1000] loss: 0.650, accuracy: 0.621\n",
            "[Epoch 1, Batch  1100] loss: 0.652, accuracy: 0.617\n",
            "[Epoch 1, Batch  1200] loss: 0.654, accuracy: 0.618\n",
            "[Epoch 1, Batch  1300] loss: 0.649, accuracy: 0.622\n",
            "[Epoch 1, Batch  1400] loss: 0.645, accuracy: 0.628\n",
            "[Epoch 1, Batch  1500] loss: 0.647, accuracy: 0.624\n",
            "[Epoch 1, Batch  1600] loss: 0.647, accuracy: 0.626\n",
            "[Epoch 1, Batch  1700] loss: 0.653, accuracy: 0.619\n",
            "[Epoch 1, Batch  1800] loss: 0.645, accuracy: 0.624\n",
            "Epoch 1 accuracy: 0.627\n",
            "Epoch 1 test accuracy: 0.629, test ROC AUC score: 0.629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2, Batch   100] loss: 0.642, accuracy: 0.630\n",
            "[Epoch 2, Batch   200] loss: 0.641, accuracy: 0.635\n",
            "[Epoch 2, Batch   300] loss: 0.638, accuracy: 0.628\n",
            "[Epoch 2, Batch   400] loss: 0.642, accuracy: 0.636\n",
            "[Epoch 2, Batch   500] loss: 0.633, accuracy: 0.645\n",
            "[Epoch 2, Batch   600] loss: 0.634, accuracy: 0.645\n",
            "[Epoch 2, Batch   700] loss: 0.628, accuracy: 0.652\n",
            "[Epoch 2, Batch   800] loss: 0.632, accuracy: 0.648\n",
            "[Epoch 2, Batch   900] loss: 0.627, accuracy: 0.650\n",
            "[Epoch 2, Batch  1000] loss: 0.625, accuracy: 0.650\n",
            "[Epoch 2, Batch  1100] loss: 0.620, accuracy: 0.660\n",
            "[Epoch 2, Batch  1200] loss: 0.628, accuracy: 0.649\n",
            "[Epoch 2, Batch  1300] loss: 0.628, accuracy: 0.649\n",
            "[Epoch 2, Batch  1400] loss: 0.624, accuracy: 0.655\n",
            "[Epoch 2, Batch  1500] loss: 0.618, accuracy: 0.662\n",
            "[Epoch 2, Batch  1600] loss: 0.621, accuracy: 0.657\n",
            "[Epoch 2, Batch  1700] loss: 0.622, accuracy: 0.660\n",
            "[Epoch 2, Batch  1800] loss: 0.610, accuracy: 0.670\n",
            "Epoch 2 accuracy: 0.659\n",
            "Epoch 2 test accuracy: 0.668, test ROC AUC score: 0.669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3, Batch   100] loss: 0.614, accuracy: 0.668\n",
            "[Epoch 3, Batch   200] loss: 0.619, accuracy: 0.663\n",
            "[Epoch 3, Batch   300] loss: 0.610, accuracy: 0.672\n",
            "[Epoch 3, Batch   400] loss: 0.612, accuracy: 0.674\n",
            "[Epoch 3, Batch   500] loss: 0.609, accuracy: 0.674\n",
            "[Epoch 3, Batch   600] loss: 0.605, accuracy: 0.678\n",
            "[Epoch 3, Batch   700] loss: 0.604, accuracy: 0.682\n",
            "[Epoch 3, Batch   800] loss: 0.610, accuracy: 0.677\n",
            "[Epoch 3, Batch   900] loss: 0.611, accuracy: 0.671\n",
            "[Epoch 3, Batch  1000] loss: 0.607, accuracy: 0.672\n",
            "[Epoch 3, Batch  1100] loss: 0.607, accuracy: 0.677\n",
            "[Epoch 3, Batch  1200] loss: 0.605, accuracy: 0.682\n",
            "[Epoch 3, Batch  1300] loss: 0.614, accuracy: 0.669\n",
            "[Epoch 3, Batch  1400] loss: 0.608, accuracy: 0.679\n",
            "[Epoch 3, Batch  1500] loss: 0.605, accuracy: 0.682\n",
            "[Epoch 3, Batch  1600] loss: 0.604, accuracy: 0.681\n",
            "[Epoch 3, Batch  1700] loss: 0.600, accuracy: 0.687\n",
            "[Epoch 3, Batch  1800] loss: 0.599, accuracy: 0.691\n",
            "Epoch 3 accuracy: 0.675\n",
            "Epoch 3 test accuracy: 0.691, test ROC AUC score: 0.691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4, Batch   100] loss: 0.604, accuracy: 0.677\n",
            "[Epoch 4, Batch   200] loss: 0.606, accuracy: 0.680\n",
            "[Epoch 4, Batch   300] loss: 0.598, accuracy: 0.681\n",
            "[Epoch 4, Batch   400] loss: 0.595, accuracy: 0.693\n",
            "[Epoch 4, Batch   500] loss: 0.588, accuracy: 0.695\n",
            "[Epoch 4, Batch   600] loss: 0.603, accuracy: 0.682\n",
            "[Epoch 4, Batch   700] loss: 0.602, accuracy: 0.684\n",
            "[Epoch 4, Batch   800] loss: 0.599, accuracy: 0.680\n",
            "[Epoch 4, Batch   900] loss: 0.592, accuracy: 0.688\n",
            "[Epoch 4, Batch  1000] loss: 0.600, accuracy: 0.683\n",
            "[Epoch 4, Batch  1100] loss: 0.591, accuracy: 0.697\n",
            "[Epoch 4, Batch  1200] loss: 0.600, accuracy: 0.684\n",
            "[Epoch 4, Batch  1300] loss: 0.592, accuracy: 0.692\n",
            "[Epoch 4, Batch  1400] loss: 0.597, accuracy: 0.691\n",
            "[Epoch 4, Batch  1500] loss: 0.587, accuracy: 0.695\n",
            "[Epoch 4, Batch  1600] loss: 0.601, accuracy: 0.680\n",
            "[Epoch 4, Batch  1700] loss: 0.600, accuracy: 0.690\n",
            "[Epoch 4, Batch  1800] loss: 0.597, accuracy: 0.690\n",
            "Epoch 4 accuracy: 0.686\n",
            "Epoch 4 test accuracy: 0.691, test ROC AUC score: 0.691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5, Batch   100] loss: 0.585, accuracy: 0.700\n",
            "[Epoch 5, Batch   200] loss: 0.595, accuracy: 0.690\n",
            "[Epoch 5, Batch   300] loss: 0.597, accuracy: 0.686\n",
            "[Epoch 5, Batch   400] loss: 0.599, accuracy: 0.678\n",
            "[Epoch 5, Batch   500] loss: 0.591, accuracy: 0.692\n",
            "[Epoch 5, Batch   600] loss: 0.582, accuracy: 0.698\n",
            "[Epoch 5, Batch   700] loss: 0.584, accuracy: 0.703\n",
            "[Epoch 5, Batch   800] loss: 0.600, accuracy: 0.685\n",
            "[Epoch 5, Batch   900] loss: 0.595, accuracy: 0.687\n",
            "[Epoch 5, Batch  1000] loss: 0.600, accuracy: 0.677\n",
            "[Epoch 5, Batch  1100] loss: 0.587, accuracy: 0.696\n",
            "[Epoch 5, Batch  1200] loss: 0.591, accuracy: 0.691\n",
            "[Epoch 5, Batch  1300] loss: 0.584, accuracy: 0.695\n",
            "[Epoch 5, Batch  1400] loss: 0.591, accuracy: 0.696\n",
            "[Epoch 5, Batch  1500] loss: 0.595, accuracy: 0.690\n",
            "[Epoch 5, Batch  1600] loss: 0.581, accuracy: 0.703\n",
            "[Epoch 5, Batch  1700] loss: 0.586, accuracy: 0.695\n",
            "[Epoch 5, Batch  1800] loss: 0.586, accuracy: 0.696\n",
            "Epoch 5 accuracy: 0.688\n",
            "Epoch 5 test accuracy: 0.699, test ROC AUC score: 0.699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6, Batch   100] loss: 0.581, accuracy: 0.699\n",
            "[Epoch 6, Batch   200] loss: 0.582, accuracy: 0.699\n",
            "[Epoch 6, Batch   300] loss: 0.589, accuracy: 0.691\n",
            "[Epoch 6, Batch   400] loss: 0.593, accuracy: 0.688\n",
            "[Epoch 6, Batch   500] loss: 0.589, accuracy: 0.694\n",
            "[Epoch 6, Batch   600] loss: 0.583, accuracy: 0.701\n",
            "[Epoch 6, Batch   700] loss: 0.584, accuracy: 0.700\n",
            "[Epoch 6, Batch   800] loss: 0.584, accuracy: 0.698\n",
            "[Epoch 6, Batch   900] loss: 0.582, accuracy: 0.697\n",
            "[Epoch 6, Batch  1000] loss: 0.585, accuracy: 0.698\n",
            "[Epoch 6, Batch  1100] loss: 0.590, accuracy: 0.690\n",
            "[Epoch 6, Batch  1200] loss: 0.586, accuracy: 0.697\n",
            "[Epoch 6, Batch  1300] loss: 0.568, accuracy: 0.714\n",
            "[Epoch 6, Batch  1400] loss: 0.575, accuracy: 0.708\n",
            "[Epoch 6, Batch  1500] loss: 0.580, accuracy: 0.707\n",
            "[Epoch 6, Batch  1600] loss: 0.587, accuracy: 0.699\n",
            "[Epoch 6, Batch  1700] loss: 0.583, accuracy: 0.700\n",
            "[Epoch 6, Batch  1800] loss: 0.580, accuracy: 0.698\n",
            "Epoch 6 accuracy: 0.685\n",
            "Epoch 6 test accuracy: 0.688, test ROC AUC score: 0.688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7, Batch   100] loss: 0.588, accuracy: 0.692\n",
            "[Epoch 7, Batch   200] loss: 0.579, accuracy: 0.701\n",
            "[Epoch 7, Batch   300] loss: 0.585, accuracy: 0.699\n",
            "[Epoch 7, Batch   400] loss: 0.585, accuracy: 0.697\n",
            "[Epoch 7, Batch   500] loss: 0.582, accuracy: 0.699\n",
            "[Epoch 7, Batch   600] loss: 0.562, accuracy: 0.717\n",
            "[Epoch 7, Batch   700] loss: 0.569, accuracy: 0.710\n",
            "[Epoch 7, Batch   800] loss: 0.578, accuracy: 0.704\n",
            "[Epoch 7, Batch   900] loss: 0.579, accuracy: 0.706\n",
            "[Epoch 7, Batch  1000] loss: 0.585, accuracy: 0.699\n",
            "[Epoch 7, Batch  1100] loss: 0.582, accuracy: 0.698\n",
            "[Epoch 7, Batch  1200] loss: 0.580, accuracy: 0.700\n",
            "[Epoch 7, Batch  1300] loss: 0.577, accuracy: 0.705\n",
            "[Epoch 7, Batch  1400] loss: 0.572, accuracy: 0.713\n",
            "[Epoch 7, Batch  1500] loss: 0.578, accuracy: 0.695\n",
            "[Epoch 7, Batch  1600] loss: 0.583, accuracy: 0.700\n",
            "[Epoch 7, Batch  1700] loss: 0.576, accuracy: 0.703\n",
            "[Epoch 7, Batch  1800] loss: 0.589, accuracy: 0.693\n",
            "Epoch 7 accuracy: 0.697\n",
            "Epoch 7 test accuracy: 0.703, test ROC AUC score: 0.703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8, Batch   100] loss: 0.563, accuracy: 0.719\n",
            "[Epoch 8, Batch   200] loss: 0.579, accuracy: 0.701\n",
            "[Epoch 8, Batch   300] loss: 0.571, accuracy: 0.711\n",
            "[Epoch 8, Batch   400] loss: 0.576, accuracy: 0.704\n",
            "[Epoch 8, Batch   500] loss: 0.570, accuracy: 0.710\n",
            "[Epoch 8, Batch   600] loss: 0.582, accuracy: 0.701\n",
            "[Epoch 8, Batch   700] loss: 0.572, accuracy: 0.700\n",
            "[Epoch 8, Batch   800] loss: 0.577, accuracy: 0.707\n",
            "[Epoch 8, Batch   900] loss: 0.573, accuracy: 0.711\n",
            "[Epoch 8, Batch  1000] loss: 0.572, accuracy: 0.707\n",
            "[Epoch 8, Batch  1100] loss: 0.580, accuracy: 0.703\n",
            "[Epoch 8, Batch  1200] loss: 0.580, accuracy: 0.702\n",
            "[Epoch 8, Batch  1300] loss: 0.571, accuracy: 0.708\n",
            "[Epoch 8, Batch  1400] loss: 0.578, accuracy: 0.706\n",
            "[Epoch 8, Batch  1500] loss: 0.581, accuracy: 0.697\n",
            "[Epoch 8, Batch  1600] loss: 0.581, accuracy: 0.709\n",
            "[Epoch 8, Batch  1700] loss: 0.576, accuracy: 0.706\n",
            "[Epoch 8, Batch  1800] loss: 0.581, accuracy: 0.703\n",
            "Epoch 8 accuracy: 0.700\n",
            "Epoch 8 test accuracy: 0.709, test ROC AUC score: 0.709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9, Batch   100] loss: 0.560, accuracy: 0.719\n",
            "[Epoch 9, Batch   200] loss: 0.576, accuracy: 0.704\n",
            "[Epoch 9, Batch   300] loss: 0.576, accuracy: 0.702\n",
            "[Epoch 9, Batch   400] loss: 0.576, accuracy: 0.709\n",
            "[Epoch 9, Batch   500] loss: 0.570, accuracy: 0.715\n",
            "[Epoch 9, Batch   600] loss: 0.571, accuracy: 0.709\n",
            "[Epoch 9, Batch   700] loss: 0.581, accuracy: 0.700\n",
            "[Epoch 9, Batch   800] loss: 0.567, accuracy: 0.714\n",
            "[Epoch 9, Batch   900] loss: 0.575, accuracy: 0.714\n",
            "[Epoch 9, Batch  1000] loss: 0.575, accuracy: 0.705\n",
            "[Epoch 9, Batch  1100] loss: 0.568, accuracy: 0.713\n",
            "[Epoch 9, Batch  1200] loss: 0.576, accuracy: 0.701\n",
            "[Epoch 9, Batch  1300] loss: 0.572, accuracy: 0.709\n",
            "[Epoch 9, Batch  1400] loss: 0.568, accuracy: 0.714\n",
            "[Epoch 9, Batch  1500] loss: 0.575, accuracy: 0.699\n",
            "[Epoch 9, Batch  1600] loss: 0.574, accuracy: 0.705\n",
            "[Epoch 9, Batch  1700] loss: 0.568, accuracy: 0.713\n",
            "[Epoch 9, Batch  1800] loss: 0.567, accuracy: 0.712\n",
            "Epoch 9 accuracy: 0.708\n",
            "Epoch 9 test accuracy: 0.706, test ROC AUC score: 0.707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10, Batch   100] loss: 0.567, accuracy: 0.708\n",
            "[Epoch 10, Batch   200] loss: 0.567, accuracy: 0.712\n",
            "[Epoch 10, Batch   300] loss: 0.563, accuracy: 0.718\n",
            "[Epoch 10, Batch   400] loss: 0.569, accuracy: 0.715\n",
            "[Epoch 10, Batch   500] loss: 0.571, accuracy: 0.707\n",
            "[Epoch 10, Batch   600] loss: 0.570, accuracy: 0.707\n",
            "[Epoch 10, Batch   700] loss: 0.573, accuracy: 0.708\n",
            "[Epoch 10, Batch   800] loss: 0.568, accuracy: 0.708\n",
            "[Epoch 10, Batch   900] loss: 0.566, accuracy: 0.716\n",
            "[Epoch 10, Batch  1000] loss: 0.566, accuracy: 0.706\n",
            "[Epoch 10, Batch  1100] loss: 0.570, accuracy: 0.708\n",
            "[Epoch 10, Batch  1200] loss: 0.569, accuracy: 0.713\n",
            "[Epoch 10, Batch  1300] loss: 0.579, accuracy: 0.698\n",
            "[Epoch 10, Batch  1400] loss: 0.571, accuracy: 0.709\n",
            "[Epoch 10, Batch  1500] loss: 0.572, accuracy: 0.705\n",
            "[Epoch 10, Batch  1600] loss: 0.573, accuracy: 0.707\n",
            "[Epoch 10, Batch  1700] loss: 0.570, accuracy: 0.712\n",
            "[Epoch 10, Batch  1800] loss: 0.575, accuracy: 0.709\n",
            "Epoch 10 accuracy: 0.715\n",
            "Epoch 10 test accuracy: 0.708, test ROC AUC score: 0.708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sDZZW1nqr9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}